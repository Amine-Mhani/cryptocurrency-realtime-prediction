{
  "paragraphs": [
    {
      "text": "%pyspark\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import from_json\r\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\r\n\r\n# Create a SparkSession\r\nspark = SparkSession.builder \\\r\n    .appName(\"KafkaToHiveORC\") \\\r\n    .config(\"spark.sql.orc.impl\", \"native\") \\\r\n    .enableHiveSupport() \\\r\n    .getOrCreate()\r\n\r\n# Kafka details\r\nkafka_server = \"kafka-broker:29092\"\r\ntopic = \"source_topic-001\"\r\n\r\nschema = StructType([\r\n    StructField(\"Name\", StringType()),\r\n    StructField(\"Price\", StringType()),\r\n    StructField(\"24H_CHANGE\", StringType()),\r\n    StructField(\"24H_VOLUME\", StringType()),\r\n    StructField(\"Market_Cap\", StringType()),\r\n    StructField(\"Datetime\", StringType()),\r\n])\r\n\r\ndf = spark.readStream \\\r\n    .format(\"kafka\") \\\r\n    .option(\"kafka.bootstrap.servers\", kafka_server) \\\r\n    .option(\"subscribe\", topic) \\\r\n    .option(\"failOnDataLoss\", \"false\") \\\r\n    .load() \\\r\n    .selectExpr(\"CAST(value AS STRING)\") \\\r\n    .select(from_json(\"value\", schema).alias(\"data\")) \\\r\n    .select(\"data.*\")\r\n\r\n   \r\ndf.writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .format(\"parquet\") \\\r\n    .option(\"path\", \"/user/hive/warehouse/crypto_data1\") \\\r\n    .option(\"checkpointLocation\", \"/tmp/myNew_checkpoint\") \\\r\n    .option(\"failOnDataLoss\", \"false\") \\\r\n    .start() \\\r\n    .awaitTermination()\r\n\r\nspark.stop()\r\n",
      "user": "anonymous",
      "dateUpdated": "2023-12-17T20:06:25+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1702734028784_2077741101",
      "id": "paragraph_1702734028784_2077741101",
      "dateCreated": "2023-12-16T13:40:28+0000",
      "dateStarted": "2023-12-17T20:06:25+0000",
      "dateFinished": "2023-12-17T11:08:52+0000",
      "status": "ABORT",
      "focus": true,
      "$$hashKey": "object:819"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2023-12-16T13:40:37+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1702734037125_100674856",
      "id": "paragraph_1702734037125_100674856",
      "dateCreated": "2023-12-16T13:40:37+0000",
      "status": "READY",
      "$$hashKey": "object:820"
    }
  ],
  "name": "sparkStream",
  "id": "2JH2WKSFJ",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/sparkStream"
}