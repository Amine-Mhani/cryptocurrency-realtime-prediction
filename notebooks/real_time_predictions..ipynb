{
  "metadata": {
    "name": "real_time_predictions",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d spark.read.parquet(\"/user/hive/warehouse/crypto_data1/part-*.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d parquet_df.drop(\"24H_VOLUME\")\nparquet_df \u003d parquet_df.drop(\"24H_CHANGE\")\nparquet_df \u003d parquet_df.drop(\"24H_CHANGE_CLEANED\")\nparquet_df \u003d parquet_df.drop(\"Market_Cap\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Filter the DataFrame for \u0027BTC\u0027 in the \u0027Name\u0027 column\nparquet_df \u003d parquet_df.filter(col(\u0027Name\u0027) \u003d\u003d \u0027BTC\u0027).orderBy(col(\u0027Datetime\u0027).desc()).limit(50)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Data Cleaning\n# Convert Price to numeric\n# Replace \"$\" and \",\" in the Price column, then cast to float\nparquet_df \u003d parquet_df.withColumn(\"Price\", regexp_replace(col(\"Price\"), \"[\\$,]\", \"\").cast(\"float\"))\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Convert Datetime column to timestamp type\nparquet_df \u003d parquet_df.withColumn(\"Datetime\", to_timestamp(\"Datetime\"))\n\n# Extract year, month, day, and hour\nparquet_df \u003d parquet_df.withColumn(\"Year\", year(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Month\", month(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Day\", dayofmonth(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Hour\", hour(\"Datetime\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d parquet_df.drop(\"Datetime\")\nparquet_df \u003d parquet_df.drop(\"Name\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Assemble features into a vector\nfeature_columns \u003d [\u0027Year\u0027, \u0027Month\u0027, \u0027Day\u0027, \u0027Hour\u0027]\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\nassembled_data \u003d assembler.transform(parquet_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import PipelineModel"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nloaded_model \u003d PipelineModel.load(\"/user/root/Linear Regression_20231218003635\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nloaded_model.transform(assembled_data).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions \u003d loaded_model.transform(assembled_data)\nevaluator \u003d RegressionEvaluator(labelCol\u003d\"Price\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\")\n\nrmse \u003d evaluator.evaluate(predictions)\nprint(\"RMSE:\", rmse)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport json\nimport os\nfrom datetime import datetime"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Example model_info\nmodel_info \u003d {\n    \"rmse\": rmse,\n    \"testing_date\": str(datetime.now())\n}\n\n# Generate a unique filename based on the current timestamp\ntimestamp \u003d datetime.now().strftime(\"%Y%m%d%H%M%S\")\nfile_name \u003d f\"test_model_info_{timestamp}.json\"  # Unique file name\n\n# Specify the path where Hive table is located\nhive_table_path \u003d \"/user/hive/warehouse/models_testing_infos_table\"\n\n# Path to save the JSON file temporarily (in the local filesystem)\nlocal_file_path \u003d f\"/tmp/{file_name}\"  # Change to your preferred temporary directory\n\n# Save model information as JSON in the local filesystem\nwith open(local_file_path, \"w\") as json_file:\n    json.dump(model_info, json_file)\n\n# Move the generated file to the Hive table directory\nos.system(f\"hdfs dfs -put {local_file_path} {hive_table_path}/{file_name}\")\n\n# Remove the temporary local file after moving to HDFS\nos.remove(local_file_path)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}