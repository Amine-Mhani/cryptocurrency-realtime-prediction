{
  "metadata": {
    "name": "machine_learning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\r\nfrom pyspark.ml.regression import RandomForestRegressor\r\nfrom pyspark.ml import Pipeline\r\nfrom pyspark.sql.functions import col, sum\r\nfrom pyspark.sql.functions import regexp_replace, col, hour, to_timestamp, udf\r\nfrom pyspark.sql.functions import year, month, dayofmonth, hour"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d spark.read.parquet(\"/user/hive/warehouse/crypto_data1/part-*.parquet\")\n\n# Perform operations on the read DataFrame parquet_df\n# For example, you can show the content of the DataFrame\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Data Cleaning\n# Convert Price to numeric\n# Replace \"$\" and \",\" in the Price column, then cast to float\nparquet_df \u003d parquet_df.withColumn(\"Price\", regexp_replace(col(\"Price\"), \"[\\$,]\", \"\").cast(\"float\"))\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Clean 24H_VOLUME and Market_Cap (Remove $ and B/M/K, convert to numeric)\ndef clean_currency(x):\n    return float(x.replace(\",\", \"\").replace(\"$\", \"\").replace(\"B\", \"\").replace(\"M\", \"\").replace(\"K\", \"\"))\n\nclean_currency_udf \u003d udf(clean_currency)\nparquet_df \u003d parquet_df.withColumn(\"24H_VOLUME\", clean_currency_udf(col(\"24H_VOLUME\")))\nparquet_df \u003d parquet_df.withColumn(\"Market_Cap\", clean_currency_udf(col(\"Market_Cap\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Function to clean 24H_CHANGE column\ndef clean_24h_change(value):\n    sign \u003d \u0027-\u0027 if \u0027-\u0027 in value else \u0027\u0027\n    cleaned_value \u003d float(value.replace(\u0027+\u0027, \u0027\u0027).replace(\u0027-\u0027, \u0027\u0027).replace(\u0027%\u0027, \u0027\u0027))\n    return float(sign + str(cleaned_value))"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Applying the function to the DataFrame\nclean_24h_change_udf \u003d udf(clean_24h_change)\n\n# Assuming parquet_df is your DataFrame\nparquet_df \u003d parquet_df.withColumn(\"24H_CHANGE_CLEANED\", clean_24h_change_udf(col(\"24H_CHANGE\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n# Drop the previous 24H_CHANGE column\r\nparquet_df \u003d parquet_df.drop(\"24H_CHANGE\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Convert Datetime column to timestamp type\nparquet_df \u003d parquet_df.withColumn(\"Datetime\", to_timestamp(\"Datetime\"))\n\n# Extract year, month, day, and hour\nparquet_df \u003d parquet_df.withColumn(\"Year\", year(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Month\", month(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Day\", dayofmonth(\"Datetime\"))\nparquet_df \u003d parquet_df.withColumn(\"Hour\", hour(\"Datetime\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d parquet_df.drop(\"Datetime\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npip install matplotlib"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nimport matplotlib.pyplot as plt\nimport io\nimport base64"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n\r\n\r\n# Filter for BTC data only\r\nbtc_df \u003d parquet_df.filter(parquet_df[\u0027Name\u0027] \u003d\u003d \u0027BTC\u0027)\r\n\r\n# Calculate mean value of BTC price each day\r\nmean_value_each_day \u003d btc_df.groupBy(\u0027Day\u0027).avg(\u0027Price\u0027).orderBy(\u0027Day\u0027).toPandas()\r\n\r\n# Plotting mean value of BTC price each day\r\nplt.figure(figsize\u003d(10, 6))\r\nplt.plot(mean_value_each_day[\u0027Day\u0027], mean_value_each_day[\u0027avg(Price)\u0027], marker\u003d\u0027o\u0027)\r\nplt.xlabel(\u0027Day\u0027)\r\nplt.ylabel(\u0027Mean BTC Price\u0027)\r\nplt.title(\u0027Mean BTC Price for Each Day\u0027)\r\nplt.grid(True)\r\n\r\n# Save the Matplotlib plot as an image\r\nimg \u003d io.BytesIO()\r\nplt.savefig(img, format\u003d\u0027png\u0027)\r\nplt.close()\r\n\r\n# Encode the image data in Base64\r\nimg.seek(0)\r\nimage_data \u003d base64.b64encode(img.getvalue()).decode()\r\n\r\n# Display the image using the %html interpreter\r\nimage_html \u003d f\u0027\u003cimg src\u003d\"data:image/png;base64,{image_data}\" alt\u003d\"Mean BTC Price\"\u003e\u0027\r\nprint(\"%html \" + image_html)\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Filter for BTC, ETH, XRP, and LTC data\nselected_names \u003d [\u0027BTC\u0027, \u0027ETH\u0027, \u0027XRP\u0027, \u0027LTC\u0027]\nselected_df \u003d parquet_df.filter(parquet_df[\u0027Name\u0027].isin(selected_names))\n\n# Group by \u0027Name\u0027, \u0027Day\u0027 and calculate mean value of \u0027Price\u0027\nmean_price_each_day \u003d selected_df.groupBy(\u0027Name\u0027, \u0027Day\u0027).avg(\u0027Price\u0027).toPandas()\n\n# Create a figure with 2x2 subplots\nfig, axs \u003d plt.subplots(2, 2, figsize\u003d(12, 8))\naxs \u003d axs.flatten()\n\n# Plotting mean value of price for each cryptocurrency\nfor i, name in enumerate(selected_names):\n    data \u003d mean_price_each_day[mean_price_each_day[\u0027Name\u0027] \u003d\u003d name]\n    axs[i].bar(data[\u0027Day\u0027], data[\u0027avg(Price)\u0027])\n    axs[i].set_title(f\"Mean Price of {name}\")\n    axs[i].set_xlabel(\u0027Day\u0027)\n    axs[i].set_ylabel(\u0027Mean Price\u0027)\n\n# Adjust layout and save the plot as an image\nplt.tight_layout()\n# Save the Matplotlib plot as an image\nimg \u003d io.BytesIO()\nplt.savefig(img, format\u003d\u0027png\u0027)\nplt.close()\n\n# Encode the image data in Base64\nimg.seek(0)\nimage_data \u003d base64.b64encode(img.getvalue()).decode()\n\n# Display the image using the %html interpreter\nimage_html \u003d f\u0027\u003cimg src\u003d\"data:image/png;base64,{image_data}\" alt\u003d\"Mean Cryptocurrency Prices\"\u003e\u0027\nprint(\"%html \" + image_html)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Calculate mean price of each cryptocurrency\nmean_price_each_name \u003d parquet_df.groupBy(\u0027Name\u0027).avg(\u0027Price\u0027).orderBy(\u0027avg(Price)\u0027).toPandas()\n\n# Plotting mean price of each cryptocurrency\nplt.figure(figsize\u003d(12, 12))\nplt.barh(mean_price_each_name[\u0027Name\u0027], mean_price_each_name[\u0027avg(Price)\u0027], color\u003d\u0027skyblue\u0027)\nplt.xlabel(\u0027Mean Price\u0027)\nplt.title(\u0027Mean Price of Each Cryptocurrency\u0027)\nplt.grid(axis\u003d\u0027x\u0027)\n\n# Save the Matplotlib plot as an image\nimg2 \u003d io.BytesIO()\nplt.savefig(img2, format\u003d\u0027png\u0027)\nplt.close()\n\n# Encode the image data in Base64\nimg2.seek(0)\nimage_data2 \u003d base64.b64encode(img2.getvalue()).decode()\n\n# Display the image using the %html interpreter\nimage_html2 \u003d f\u0027\u003cimg src\u003d\"data:image/png;base64,{image_data2}\" alt\u003d\"Mean Price of Each Cryptocurrency\"\u003e\u0027\nprint(\"%html \" + image_html2)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Using printSchema() to display the schema\nparquet_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df \u003d parquet_df.drop(\"24H_VOLUME\")\nparquet_df \u003d parquet_df.drop(\"24H_CHANGE_CLEANED\")\nparquet_df \u003d parquet_df.drop(\"Market_Cap\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nparquet_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Filter the DataFrame for \u0027BTC\u0027 in the \u0027Name\u0027 column\nbitcoin_df \u003d parquet_df.filter(col(\u0027Name\u0027) \u003d\u003d \u0027BTC\u0027)\n\n# Show the resulting DataFrame\nbitcoin_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbitcoin_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbitcoin_df \u003d bitcoin_df.drop(\"Name\")"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbitcoin_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Machine Learning\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Assemble features into a vector\nfeature_columns \u003d [\u0027Year\u0027, \u0027Month\u0027, \u0027Day\u0027, \u0027Hour\u0027]\nassembler \u003d VectorAssembler(inputCols\u003dfeature_columns, outputCol\u003d\u0027features\u0027)\nassembled_data \u003d assembler.transform(bitcoin_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Split data into training and testing sets\ntrain_data, test_data \u003d assembled_data.randomSplit([0.8, 0.2], seed\u003d42)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Initialize regressors\nlr \u003d LinearRegression(labelCol\u003d\u0027Price\u0027, featuresCol\u003d\u0027features\u0027)\ndt \u003d DecisionTreeRegressor(labelCol\u003d\u0027Price\u0027, featuresCol\u003d\u0027features\u0027)\nrf \u003d RandomForestRegressor(labelCol\u003d\u0027Price\u0027, featuresCol\u003d\u0027features\u0027)\ngbt \u003d GBTRegressor(labelCol\u003d\u0027Price\u0027, featuresCol\u003d\u0027features\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Train models\nlr_model \u003d lr.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndt_model \u003d dt.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nrf_model \u003d rf.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ngbt_model \u003d gbt.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Make predictions\nlr_predictions \u003d lr_model.transform(test_data)\ndt_predictions \u003d dt_model.transform(test_data)\nrf_predictions \u003d rf_model.transform(test_data)\ngbt_predictions \u003d gbt_model.transform(test_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Evaluate models\nevaluator \u003d RegressionEvaluator(labelCol\u003d\u0027Price\u0027, predictionCol\u003d\u0027prediction\u0027, metricName\u003d\u0027rmse\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlr_rmse \u003d evaluator.evaluate(lr_predictions)\ndt_rmse \u003d evaluator.evaluate(dt_predictions)\nrf_rmse \u003d evaluator.evaluate(rf_predictions)\ngbt_rmse \u003d evaluator.evaluate(gbt_predictions)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nprint(f\"Linear Regression RMSE: {lr_rmse}\")\nprint(f\"Decision Tree RMSE: {dt_rmse}\")\nprint(f\"Random Forest RMSE: {rf_rmse}\")\nprint(f\"GBT RMSE: {gbt_rmse}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef get_best_model(lr_rmse, dt_rmse, rf_rmse, gbt_rmse):\n    models \u003d {\u0027Linear Regression\u0027: lr_rmse, \u0027Decision Tree\u0027: dt_rmse, \u0027Random Forest\u0027: rf_rmse, \u0027GBT\u0027: gbt_rmse}\n    best_model \u003d min(models, key\u003dmodels.get)\n    return best_model"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbest_model_name \u003d get_best_model(lr_rmse, dt_rmse, rf_rmse, gbt_rmse)\nprint(f\"The best model is: {best_model_name}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Save best model"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport json\nimport os\nfrom datetime import datetime"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Example model_info\nmodel_info \u003d {\n    \"name\": best_model_name,\n    \"training_date\": str(datetime.now())\n}\n\n# Generate a unique filename based on the current timestamp\ntimestamp \u003d datetime.now().strftime(\"%Y%m%d%H%M%S\")\nfile_name \u003d f\"best_model_info_{timestamp}.json\"  # Unique file name\n\n# Specify the path where Hive table is located\nhive_table_path \u003d \"/user/hive/warehouse/models_infos_table\"\n\n# Path to save the JSON file temporarily (in the local filesystem)\nlocal_file_path \u003d f\"/tmp/{file_name}\"  # Change to your preferred temporary directory\n\n# Save model information as JSON in the local filesystem\nwith open(local_file_path, \"w\") as json_file:\n    json.dump(model_info, json_file)\n\n# Move the generated file to the Hive table directory\nos.system(f\"hdfs dfs -put {local_file_path} {hive_table_path}/{file_name}\")\n\n# Remove the temporary local file after moving to HDFS\nos.remove(local_file_path)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Save the best model as part of a pipeline\nif best_model_name \u003d\u003d \u0027Random Forest\u0027:\n    best_model \u003d rf_model\nelif best_model_name \u003d\u003d \u0027Decision Tree\u0027:\n    best_model \u003d dt_model\nelif best_model_name \u003d\u003d \u0027Linear Regression\u0027:\n    best_model \u003d lr_model\nelse:\n    best_model \u003d gbt_model"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n# Create a pipeline and add the model to it\npipeline \u003d PipelineModel(stages\u003d[best_model])\n\nmodel_folder \u003d f\"{best_model_name}_{timestamp}\"\n# Save the pipeline model\npipeline.save(model_folder)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}