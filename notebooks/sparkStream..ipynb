{
  "metadata": {
    "name": "sparkStream",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import from_json\r\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\r\n\r\n# Create a SparkSession\r\nspark \u003d SparkSession.builder \\\r\n    .appName(\"KafkaToHiveORC\") \\\r\n    .config(\"spark.sql.orc.impl\", \"native\") \\\r\n    .enableHiveSupport() \\\r\n    .getOrCreate()\r\n\r\n# Kafka details\r\nkafka_server \u003d \"kafka-broker:29092\"\r\ntopic \u003d \"source_topic-001\"\r\n\r\nschema \u003d StructType([\r\n    StructField(\"Name\", StringType()),\r\n    StructField(\"Price\", StringType()),\r\n    StructField(\"24H_CHANGE\", StringType()),\r\n    StructField(\"24H_VOLUME\", StringType()),\r\n    StructField(\"Market_Cap\", StringType()),\r\n    StructField(\"Datetime\", StringType()),\r\n])\r\n\r\ndf \u003d spark.readStream \\\r\n    .format(\"kafka\") \\\r\n    .option(\"kafka.bootstrap.servers\", kafka_server) \\\r\n    .option(\"subscribe\", topic) \\\r\n    .option(\"failOnDataLoss\", \"false\") \\\r\n    .load() \\\r\n    .selectExpr(\"CAST(value AS STRING)\") \\\r\n    .select(from_json(\"value\", schema).alias(\"data\")) \\\r\n    .select(\"data.*\")\r\n\r\n   \r\ndf.writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .format(\"parquet\") \\\r\n    .option(\"path\", \"/user/hive/warehouse/crypto_data1\") \\\r\n    .option(\"checkpointLocation\", \"/tmp/myNew_checkpoint\") \\\r\n    .option(\"failOnDataLoss\", \"false\") \\\r\n    .start() \\\r\n    .awaitTermination()\r\n\r\nspark.stop()\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}